{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers"
      ],
      "metadata": {
        "id": "9dioD_mOV25m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from simpletransformers.ner import NERModel, NERArgs\n",
        "import logging"
      ],
      "metadata": {
        "id": "DMxxKWODGQmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/Final_ner_train.xlsx\")\n",
        "features = df\n",
        "features = features.sample(frac=1, random_state=42)\n",
        "split_index = int(len(features) * 0.8)\n",
        "train_data = features[:split_index]\n",
        "valid_data = features[split_index:]"
      ],
      "metadata": {
        "id": "GlIThYV2VyAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainset\n",
        "train_lst = []\n",
        "valid_lst = []\n",
        "for k in train_data.index:\n",
        "  id = train_data['ID'][k].replace('[','').replace(']','').split(\",\")\n",
        "  token = train_data['Tokenizer'][k]\n",
        "  token = ast.literal_eval(token)\n",
        "  for i in range(len(id)):\n",
        "    train_lst.append([k,token[i],str(int(id[i]))])"
      ],
      "metadata": {
        "id": "fX2rc-DkGdXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validset\n",
        "for k in valid_data.index:\n",
        "  id = valid_data['ID'][k].replace('[','').replace(']','').split(\",\")\n",
        "  token = valid_data['Tokenizer'][k]\n",
        "  token = ast.literal_eval(token)\n",
        "  for i in range(len(id)):\n",
        "    valid_lst.append([k,token[i],str(int(id[i]))])"
      ],
      "metadata": {
        "id": "EVjlmAj6eW5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_NER_TAGS = ['0','1']"
      ],
      "metadata": {
        "id": "mKHupQIyfwwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "train_data = train_lst\n",
        "train_data = pd.DataFrame(\n",
        "    train_data, columns=[\"sentence_id\", \"words\", \"labels\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ah2VCKfiii-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data = valid_lst\n",
        "eval_data = pd.DataFrame(\n",
        "    eval_data, columns=[\"sentence_id\", \"words\", \"labels\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "mH41f1Vsik37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data"
      ],
      "metadata": {
        "id": "bfQALVaU4UdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_args = NERArgs()\n",
        "ner_args.learning_rate = 1e-4\n",
        "ner_args.train_batch_size = 32 \n",
        "ner_args.use_multiprocessing = True \n",
        "ner_args.evaluate_during_training = True\n",
        "ner_args.overwrite_output_di4r = True\n",
        "ner_args.num_train_epochs = 30\n",
        "ner_args.max_seq_length = 250\n",
        "ner_args.save_model_every_epoch = False\n",
        "#\"camembert\", \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        "#\"roberta\", \"papluca/xlm-roberta-base-language-detection\"\n",
        "model = NERModel(\n",
        "     \"camembert\", \"airesearch/wangchanberta-base-att-spm-uncased\", args=ner_args, labels=_NER_TAGS\n",
        ")"
      ],
      "metadata": {
        "id": "PFhWiob3fy3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_model(\n",
        "    train_data,\n",
        "    eval_data=eval_data\n",
        ")"
      ],
      "metadata": {
        "id": "ceEavZtGf1zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_loss = \n",
        "validation_loss =\n",
        "\n",
        "epochs = range(1, len(training_loss) + 1)\n",
        "\n",
        "plt.plot(epochs, training_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, validation_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7VWBVRdFbDoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/Final_ner_test.xlsx\")\n",
        "test_lst = []\n",
        "# trainset\n",
        "for k in df.index:\n",
        "  id = df['ID'][k].replace('[','').replace(']','').split(\",\")\n",
        "  token = df['Tokenizer'][k]\n",
        "  token = ast.literal_eval(token)\n",
        "  for i in range(len(id)):\n",
        "    test_lst.append([k,token[i],str(int(id[i]))])\n",
        "\n",
        "\n",
        "  test_data = test_lst\n",
        "test_data = pd.DataFrame(\n",
        "    test_data, columns=[\"sentence_id\", \"words\", \"labels\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "0aNxNe4dFa2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, model_outputs, preds_list = model.eval_model(test_data)"
      ],
      "metadata": {
        "id": "YGj096RTkAco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "x=0\n",
        "k=0\n",
        "for i in range(len(preds_list)):\n",
        "  for k in range(len(preds_list[i])):\n",
        "    if (preds_list[i][k] == test_data['labels'][x]):\n",
        "      correct+=1\n",
        "    x+=1\n",
        "\n",
        "print(correct/x)"
      ],
      "metadata": {
        "id": "IfFaw-zKHayH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, model_outputs, preds_list = model.eval_model(eval_data)\n",
        "\n",
        "# Make predictions with the model\n",
        "predictions, raw_outputs = model.predict([\"ตู้เหล็กแบบ 2 บาน จำนวน 2 ตู้ โดยมีคุณลักษณะ ดังนี้ (1) มีมือจับชนิดผลัก (2) มีแผ่นชั้นปรับระดับ 3 ชิ้น (3) ฐานของตู้เหล็กมีความยาว 2 เมตร (4) มีระบบ noice system (5) สามารถกันนิวเคลียร์ได้ (6) มีลายกระหนก\"])\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "jQoTfXQTf6wt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
